Dressing is one of the most common activities in human society. Perfecting the skill of dressing can take an average child three to four years of daily practice. The challenge is primarily due to the combined difficulty in coordinating different body parts and manipulating soft and deformable objects (clothes). We present a technique to synthesize human dressing from an approximated input human motion (\eg a few keyframes) and a cloth simulator. We identified a set of \emph{primitive actions} account for the vast majority of the motions that a person goes through to put on an article of clothing. These primitive actions can be recombined to create a variety of motion sequences for dressing different garments with different styles. We developed a feedback dressing controller to handle each of the primitive actions. The controller plans an optimal path to achieve the action goal while making constant adjustments locally based on the current state of the simulated cloth. We demonstrated that the feedback controller is versatile to dress different clothing types, including a jacket, a pair of pants, a robe, and a vest. Our controller is also robust to different cloth mesh resolutions, which can result in significantly different simulation cloth motions. In addition, we showed that the same controller can be extended to assistive dressing.

% Because the human dressing motion is difficult to animate or motion capture, the input motion does not need to be exact or complete (a few keyframes or pretend-dressing mocap). We develop a feedback controller that takes into account the state of cloth
