\section{Limitations}

Even though our system has produced a number of successful dressing
animations, our current approach has some limitations.  One such
limitation is due to the nature of the feedback from the cloth position.
In our system, this feedback is performed using visibility calculations.
When the control system is guiding a hand to enter a sleeve, this is done
by finding the closest point on the cloth to the sleeve entry that is
visible to the tip of the hand.  It is probable that when real people
dress themselves, much of the feedback about the cloth position is due
to tactile feedback, instead of from visual information.

Another limitation of our dressing controller is that it uses kinematic
motion instead of calculating the dynamics of the human body.  This means
that the simulated human has no notion of balance, and thus may carry out
physically impossible motions.  This is especially important in dressing
tasks such as putting on a pair of pants while standing up.  The lack of
balance control could also have more subtle effects on the stance of the
person during upper body dressing.

A final limitation of our system is that it requires the user to specify
the set of actions for a given dressing task.  When given a new garment,
the system has no way of determining a sequence of actions that will
successfully place it on a body.  We can imagine a more sophisticated
system that would analyze an entirely new garment and form a plan of
actions to put on the piece of clothing.  

\karen{Perhaps we can add a paragraph about initial state of
  cloth. Currently, we start the simulation with the cloth in
  hand. Roboticists have been trying to fold laundry from random
  initial cloth configurations.}
